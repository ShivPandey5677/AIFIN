{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fbfeeb-acbf-42c7-b7a7-762266e0574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical news data for tata-consultancy-services from 2020-01-01 to 2025-01-01...\n",
      "\n",
      "Error fetching MoneyControl news: 400 - <HTML><HEAD><TITLE>Error</TITLE></HEAD><BODY>\n",
      "An error occurred while processing your request.<p>\n",
      "Reference&#32;&#35;253&#46;c825c017&#46;1741128523&#46;1ed497b\n",
      "<P>https&#58;&#47;&#47;errors&#46;edgesuite&#46;net&#47;253&#46;c825c017&#46;1741128523&#46;1ed497b</P>\n",
      "</BODY></HTML>\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't compare offset-naive and offset-aware datetimes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Data saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_historical_news_sentiment.csv & \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_twitter_sentiment.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Run for TCS (Change for other stocks)\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtata-consultancy-services\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2020-01-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025-01-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 110\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(stock_name, start_date_str, end_date_str)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Fetch financial news\u001b[39;00m\n\u001b[0;32m    109\u001b[0m moneycontrol_news \u001b[38;5;241m=\u001b[39m get_moneycontrol_news(stock_name, start_date, end_date)\n\u001b[1;32m--> 110\u001b[0m et_news \u001b[38;5;241m=\u001b[39m \u001b[43mget_economic_times_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Fetch Twitter sentiment\u001b[39;00m\n\u001b[0;32m    113\u001b[0m twitter_sentiment \u001b[38;5;241m=\u001b[39m get_twitter_sentiment(stock_name)\n",
      "Cell \u001b[1;32mIn[5], line 61\u001b[0m, in \u001b[0;36mget_economic_times_news\u001b[1;34m(start_date, end_date)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if date format is not as expected\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Filter by date range\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstart_date\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m:\n\u001b[0;32m     62\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m analyze_sentiment(entry\u001b[38;5;241m.\u001b[39mtitle)\n\u001b[0;32m     63\u001b[0m     news_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mtitle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: date_str, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mlink, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentiment})\n",
      "\u001b[1;31mTypeError\u001b[0m: can't compare offset-naive and offset-aware datetimes"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "import tweepy\n",
    "import pytz  # Import pytz for timezone handling\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# --- 1. Scrape News from MoneyControl ---\n",
    "def get_moneycontrol_news(stock_name, start_date, end_date):\n",
    "    base_url = f\"https://www.moneycontrol.com/news/tags/{stock_name}.html\"\n",
    "    headers = {\n",
    "        \"User  -Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching MoneyControl news: {response.status_code} - {response.text}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('li', class_=\"clearfix\")\n",
    "\n",
    "    news_data = []\n",
    "    for article in articles:\n",
    "        title = article.find(\"h2\").text.strip()\n",
    "        link = article.find(\"a\")[\"href\"]\n",
    "        date_str = article.find(\"span\", class_=\"date\").text.strip() if article.find(\"span\", class_=\"date\") else \"Unknown\"\n",
    "        \n",
    "        # Convert date string to datetime object\n",
    "        try:\n",
    "            date = datetime.datetime.strptime(date_str, \"%d %b %Y\")\n",
    "        except ValueError:\n",
    "            continue  # Skip if date format is not as expected\n",
    "\n",
    "        # Filter by date range\n",
    "        if start_date <= date <= end_date:\n",
    "            sentiment = analyze_sentiment(title)\n",
    "            news_data.append({\"title\": title, \"date\": date_str, \"link\": link, \"sentiment\": sentiment})\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# --- 2. Fetch Economic Times RSS Feeds ---\n",
    "def get_economic_times_news(start_date, end_date):\n",
    "    rss_url = \"https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms\"\n",
    "    feed = feedparser.parse(rss_url)\n",
    "\n",
    "    news_data = []\n",
    "    for entry in feed.entries:\n",
    "        date_str = entry.published\n",
    "        try:\n",
    "            date = datetime.datetime.strptime(date_str, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "        except ValueError:\n",
    "            continue  # Skip if date format is not as expected\n",
    "\n",
    "        # Filter by date range\n",
    "        if start_date <= date <= end_date:\n",
    "            sentiment = analyze_sentiment(entry.title)\n",
    "            news_data.append({\"title\": entry.title, \"date\": date_str, \"link\": entry.link, \"sentiment\": sentiment})\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# --- 3. Fetch Tweets for Stock Sentiment ---\n",
    "def get_twitter_sentiment(stock_symbol):\n",
    "    BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAA3MzgEAAAAARu1kKjoQ%2F1wWbgM2%2FN3bAOy9Phc%3DJ1iB1ZETpLEihEb6fmdqfuvj8KpsxxZ53BXbQAY5pichL3I5FO\"  # Replace with your actual Bearer Token\n",
    "    client = tweepy.Client(BEARER_TOKEN)\n",
    "    query = f\"{stock_symbol} stock news lang:en -is:retweet\"\n",
    "    \n",
    "    try:\n",
    "        tweets = client.search_recent_tweets(query=query, max_results=10)\n",
    "    except tweepy.TweepyException as e:\n",
    "        print(f\"Error fetching Twitter data: {e}\")\n",
    "        return []\n",
    "\n",
    "    twitter_data = []\n",
    "    if tweets.data:\n",
    "        for tweet in tweets.data:\n",
    "            sentiment = analyze_sentiment(tweet.text)\n",
    "            twitter_data.append({\"tweet\": tweet.text, \"sentiment\": sentiment})\n",
    "\n",
    "    return twitter_data\n",
    "\n",
    "# --- 4. Perform Sentiment Analysis ---\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    vader_score = analyzer.polarity_scores(text)\n",
    "    \n",
    "    sentiment = \"Neutral\"\n",
    "    if vader_score['compound'] > 0.05:\n",
    "        sentiment = \"Positive\"\n",
    "    elif vader_score['compound'] < -0.05:\n",
    "        sentiment = \"Negative\"\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "# --- 5. Main Execution ---\n",
    "def main(stock_name, start_date_str, end_date_str):\n",
    "    # Convert string dates to datetime objects and make them timezone-aware\n",
    "    tz = pytz.timezone('Asia/Kolkata')  # Set the timezone to Indian Standard Time\n",
    "    start_date = tz.localize(datetime.datetime.strptime(start_date_str, \"%Y-%m-%d\"))\n",
    "    end_date = tz.localize(datetime.datetime.strptime(end_date_str, \"%Y-%m-%d\"))\n",
    "\n",
    "    print(f\"Fetching historical news data for {stock_name} from {start_date_str} to {end_date_str}...\\n\")\n",
    "\n",
    "    # Fetch financial news\n",
    "    moneycontrol_news = get_moneycontrol_news(stock_name, start_date, end_date)\n",
    "    et_news = get_economic_times_news(start_date, end_date)\n",
    "    \n",
    "    # Fetch Twitter sentiment\n",
    "    twitter_sentiment = get_twitter_sentiment(stock_name)\n",
    "    \n",
    "    # Combine all data\n",
    "    all_news = moneycontrol_news + et_news\n",
    "    df_news = pd.DataFrame(all_news)\n",
    "    df_twitter = pd.DataFrame(twitter_sentiment)\n",
    "\n",
    "    # Save results\n",
    "    df_news.to_csv(f\"{stock_name}_historical_news_sentiment.csv\", index=False)\n",
    "    df_twitter.to_csv(f\"{stock_name}_twitter_sentiment.csv\", index=False)\n",
    "\n",
    "    print(f\"✅ Data saved: {stock_name}_historical_news_sentiment.csv & {stock_name}_twitter_sentiment.csv\")\n",
    "\n",
    "# Run for TCS (Change for other stocks)\n",
    "main(\"tata-consultancy-services\", \"2020-01-01\", \"2025-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d3eb3-a98d-4c37-be18-48478f2db8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
