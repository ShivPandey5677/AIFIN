{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62cdd0ed-e751-4703-8acb-dd2e61277d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (4.12.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\python311\\lib\\site-packages (from newspaper3k) (10.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\python311\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (5.2.2)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\python311\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (2.32.1)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (5.1.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python311\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\python311\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python311\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python311\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "093ffd86-4e2a-49a8-b164-7aa63717b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 429: Too Many Requests\n",
      "Sleeping for 9 seconds to avoid rate limits...\n",
      "Sleeping for 10 seconds to avoid rate limits...\n",
      "HTTP Error 429: Too Many Requests\n",
      "Sleeping for 6 seconds to avoid rate limits...\n",
      "Sleeping for 10 seconds to avoid rate limits...\n",
      "HTTP Error 429: Too Many Requests\n",
      "Sleeping for 13 seconds to avoid rate limits...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m all_news \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stock \u001b[38;5;129;01min\u001b[39;00m df_selected_stocks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompany Name\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 58\u001b[0m     all_news\u001b[38;5;241m.\u001b[39mextend(\u001b[43mfetch_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     59\u001b[0m     wait()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Save news data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mfetch_news\u001b[1;34m(stock_name)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_news\u001b[39m(stock_name):\n\u001b[0;32m     30\u001b[0m     google_news\u001b[38;5;241m.\u001b[39msearch(stock_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m stock news India\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add delay to prevent rate limit\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     news_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m google_news\u001b[38;5;241m.\u001b[39mresults():\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mwait\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m delay \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m15\u001b[39m)  \u001b[38;5;66;03m# Random delay between 5 to 15 seconds\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleeping for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to avoid rate limits...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search  # Alternative to GoogleNews API\n",
    "from fake_useragent import UserAgent\n",
    "import tweepy\n",
    "from selenium import webdriver\n",
    "\n",
    "# Load previously selected top 250 stocks\n",
    "df_selected_stocks = pd.read_csv(\"top_250_stocks.csv\")\n",
    "\n",
    "# Create directories for saving files\n",
    "import os\n",
    "os.makedirs(\"sentiment_data\", exist_ok=True)\n",
    "\n",
    "# Function to introduce long random delays\n",
    "def wait():\n",
    "    delay = random.randint(30, 60)  # Longer delay (30-60 sec) to prevent bans\n",
    "    print(f\"Sleeping for {delay} seconds to avoid detection...\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "# ---------------- STEP 1: FETCH FINANCIAL NEWS USING GOOGLE SEARCH ---------------- #\n",
    "\n",
    "def fetch_google_news(stock_name):\n",
    "    query = f\"{stock_name} stock news site:moneycontrol.com OR site:economictimes.indiatimes.com OR site:bloombergquint.com\"\n",
    "    \n",
    "    news_data = []\n",
    "    headers = {\"User-Agent\": UserAgent().random}  # Use random user-agent\n",
    "    \n",
    "    try:\n",
    "        # Search Google for top news articles\n",
    "        search_results = list(search(query, num_results=5, lang=\"en\"))  # Get top 5 links\n",
    "        \n",
    "        for link in search_results:\n",
    "            response = requests.get(link, headers=headers, timeout=10)  # Fetch page\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            title = soup.title.string if soup.title else \"No Title\"\n",
    "            news_data.append({\n",
    "                \"Company Name\": stock_name,\n",
    "                \"ISIN Code\": df_selected_stocks.loc[df_selected_stocks[\"Company Name\"] == stock_name, \"ISIN Code\"].values[0],\n",
    "                \"News URL\": link,\n",
    "                \"News Title\": title\n",
    "            })\n",
    "        \n",
    "        return news_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {stock_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Loop through selected stocks and fetch news\n",
    "all_news = []\n",
    "for stock in df_selected_stocks[\"Company Name\"]:\n",
    "    all_news.extend(fetch_google_news(stock))\n",
    "    wait()  # Longer wait to prevent 429\n",
    "\n",
    "# Save news data\n",
    "df_news = pd.DataFrame(all_news)\n",
    "df_news.to_csv(\"sentiment_data/long_term_news_data.csv\", index=False)\n",
    "print(\"✅ Long-Term News Data Saved Successfully!\")\n",
    "\n",
    "# ---------------- STEP 2: SCRAPE NSE COMPANY FILINGS ---------------- #\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in background\n",
    "options.add_argument(f\"user-agent={UserAgent().random}\")  # Rotate user agents\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "def get_company_filings(stock_name):\n",
    "    url = f\"https://www.nseindia.com/companies-listing/corporate-filings/{stock_name}\"\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        wait()  # Random delay\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        reports = soup.find_all(\"div\", class_=\"report-item\")\n",
    "\n",
    "        filings = []\n",
    "        for report in reports[:10]:  # Get last 10 years' filings\n",
    "            filings.append({\n",
    "                \"Company Name\": stock_name,\n",
    "                \"ISIN Code\": df_selected_stocks.loc[df_selected_stocks[\"Company Name\"] == stock_name, \"ISIN Code\"].values[0],\n",
    "                \"Date\": report.find(\"span\", class_=\"date\").text.strip(),\n",
    "                \"Title\": report.find(\"h3\").text.strip(),\n",
    "                \"Link\": report.find(\"a\")[\"href\"]\n",
    "            })\n",
    "\n",
    "        return filings\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching filings for {stock_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get filings for selected stocks\n",
    "all_filings = []\n",
    "for stock in df_selected_stocks[\"Company Name\"]:\n",
    "    all_filings.extend(get_company_filings(stock))\n",
    "    wait()\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save company filings data\n",
    "df_filings = pd.DataFrame(all_filings)\n",
    "df_filings.to_csv(\"sentiment_data/historical_company_filings.csv\", index=False)\n",
    "print(\"✅ Company Filings Data Saved Successfully!\")\n",
    "\n",
    "# ---------------- STEP 3: FETCH SOCIAL MEDIA SENTIMENT FROM TWITTER ---------------- #\n",
    "\n",
    "# Twitter API Credentials (Get from developer.twitter.com)\n",
    "BEARER_TOKEN = \"your_bearer_token\"\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "def fetch_tweets(stock_symbol):\n",
    "    query = f\"{stock_symbol} stock -is:retweet lang:en\"\n",
    "    \n",
    "    try:\n",
    "        tweets = client.search_recent_tweets(query=query, max_results=100)\n",
    "        tweet_data = []\n",
    "        \n",
    "        for tweet in tweets.data:\n",
    "            tweet_data.append({\n",
    "                \"Company Name\": stock_symbol,\n",
    "                \"ISIN Code\": df_selected_stocks.loc[df_selected_stocks[\"Company Name\"] == stock_symbol, \"ISIN Code\"].values[0],\n",
    "                \"Date\": tweet.created_at,\n",
    "                \"Tweet\": tweet.text\n",
    "            })\n",
    "\n",
    "        return tweet_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tweets for {stock_symbol}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch tweets for selected stocks\n",
    "all_tweets = []\n",
    "for stock in df_selected_stocks[\"Company Name\"]:\n",
    "    all_tweets.extend(fetch_tweets(stock))\n",
    "    wait()\n",
    "\n",
    "# Save Twitter sentiment data\n",
    "df_tweets = pd.DataFrame(all_tweets)\n",
    "df_tweets.to_csv(\"sentiment_data/long_term_twitter_sentiment.csv\", index=False)\n",
    "print(\"✅ Long-Term Twitter Sentiments Saved Successfully!\")\n",
    "\n",
    "# ---------------- FINAL OUTPUT ---------------- #\n",
    "print(\"\\n✅ Sentiment Analysis Data Collection Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdb625-a6c7-4d29-93b0-f1307c05684c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
