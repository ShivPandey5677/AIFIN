{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff17ce16-83c6-4802-b894-8c9637b74f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cookies extracted and session initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skipping 360 ONE WAM Ltd.,already processed\n",
      " Skipping AU Small Finance Bank Ltd.,already processed\n",
      " Skipping Aadhar Housing Finance Ltd.,already processed\n",
      " Skipping Aavas Financiers Ltd.,already processed\n",
      " Skipping Aditya Birla Capital Ltd.,already processed\n",
      " Skipping Aditya Birla Sun Life AMC Ltd.,already processed\n",
      " Skipping Anand Rathi Wealth Ltd.,already processed\n",
      " Skipping Angel One Ltd.,already processed\n",
      " Skipping Aptus Value Housing Finance India Ltd.,already processed\n",
      " Skipping Axis Bank Ltd.,already processed\n",
      " Skipping BSE Ltd.,already processed\n",
      " Skipping Bajaj Finance Ltd.,already processed\n",
      " Skipping Bajaj Finserv Ltd.,already processed\n",
      " Skipping Bajaj Holdings & Investment Ltd.,already processed\n",
      " Skipping Bandhan Bank Ltd.,already processed\n",
      " Skipping Bank of Baroda,already processed\n",
      " Skipping Bank of India,already processed\n",
      " Skipping Bank of Maharashtra,already processed\n",
      " Skipping CRISIL Ltd.,already processed\n",
      " Skipping Can Fin Homes Ltd.,already processed\n",
      " Skipping Canara Bank,already processed\n",
      " Skipping Capri Global Capital Ltd.,already processed\n",
      " Skipping Central Bank of India,already processed\n",
      " Skipping Central Depository Services (India) Ltd.,already processed\n",
      " Skipping Cholamandalam Financial Holdings Ltd.,already processed\n",
      " Skipping Cholamandalam Investment and Finance Company Ltd.,already processed\n",
      " Skipping City Union Bank Ltd.,already processed\n",
      " Skipping Computer Age Management Services Ltd.,already processed\n",
      " Skipping CreditAccess Grameen Ltd.,already processed\n",
      " Skipping Equitas Small Finance Bank Ltd.,already processed\n",
      " Skipping Affle (India) Ltd.,already processed\n",
      " Skipping Birlasoft Ltd.,already processed\n",
      " Skipping C.E. Info Systems Ltd.,already processed\n",
      " Skipping Coforge Ltd.,already processed\n",
      " Skipping Cyient Ltd.,already processed\n",
      " Skipping HCL Technologies Ltd.,already processed\n",
      " Skipping Happiest Minds Technologies Ltd.,already processed\n",
      " Skipping Infosys Ltd.,already processed\n",
      " Skipping Intellect Design Arena Ltd.,already processed\n",
      " Skipping KPIT Technologies Ltd.,already processed\n",
      " Skipping L&T Technology Services Ltd.,already processed\n",
      " Skipping LTIMindtree Ltd.,already processed\n",
      " Skipping Latent View Analytics Ltd.,already processed\n",
      " Skipping Mastek Ltd.,already processed\n",
      " Skipping MphasiS Ltd.,already processed\n",
      " Skipping Netweb Technologies India Ltd.,already processed\n",
      " Skipping Newgen Software Technologies Ltd.,already processed\n",
      " Skipping Oracle Financial Services Software Ltd.,already processed\n",
      " Skipping Persistent Systems Ltd.,already processed\n",
      " Skipping Sonata Software Ltd.,already processed\n",
      " Skipping Tanla Platforms Ltd.,already processed\n",
      " Skipping Tata Consultancy Services Ltd.,already processed\n",
      " Skipping Tata Elxsi Ltd.,already processed\n",
      " Skipping Tata Technologies Ltd.,already processed\n",
      "‚úÖ Downloaded: nse_annual_reports\\Tech_Mahindra_Ltd_AR\\AR.pdf\n",
      "üìÑ Directly saved PDF: Tech_Mahindra_Ltd_AR_24212_TECHM_2023_2024_27062024232127.pdf\n",
      "‚úÖ Downloaded: nse_annual_reports\\Tech_Mahindra_Ltd_AR\\AR.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                        | 55/179 [00:29<01:07,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Extracted: nse_annual_reports\\Tech_Mahindra_Ltd_AR\\AR.pdf to nse_annual_reports\\Tech_Mahindra_Ltd_AR\n",
      "üìÑ Saved PDF: Tech_Mahindra_Ltd_AR.pdf\n",
      "‚ö†Ô∏è Skipping Tech Mahindra Ltd. due to unexpected error: [WinError 183] Cannot create a file when that file already exists: 'nse_annual_reports\\\\Tech_Mahindra_Ltd_AR\\\\AR_22069_TECHM_2022_2023_26062023151955.pdf' -> 'nse_annual_reports\\\\Tech_Mahindra_Ltd_AR\\\\Tech_Mahindra_Ltd_AR_22069_TECHM_2022_2023_26062023151955.pdf'\n",
      " Skipping Abbott India Ltd.,already processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                       | 57/179 [00:40<01:33,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded: nse_annual_reports\\Ajanta_Pharmaceuticals_Ltd_AR\\AR.pdf\n",
      "‚ö†Ô∏è Skipping Ajanta Pharmaceuticals Ltd. due to unexpected error: [WinError 183] Cannot create a file when that file already exists: 'nse_annual_reports\\\\Ajanta_Pharmaceuticals_Ltd_AR\\\\AR.pdf' -> 'nse_annual_reports\\\\Ajanta_Pharmaceuticals_Ltd_AR\\\\Ajanta_Pharmaceuticals_Ltd_AR_25980_AJANTPHARM_2023_2024_0609202416057.pdf'\n",
      " Skipping Akums Drugs and Pharmaceuticals Ltd.,already processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                      | 59/179 [00:45<01:47,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded: nse_annual_reports\\Alembic_Pharmaceuticals_Ltd_AR\\AR.pdf\n",
      "‚ö†Ô∏è Skipping Alembic Pharmaceuticals Ltd. due to unexpected error: [WinError 183] Cannot create a file when that file already exists: 'nse_annual_reports\\\\Alembic_Pharmaceuticals_Ltd_AR\\\\AR.pdf' -> 'nse_annual_reports\\\\Alembic_Pharmaceuticals_Ltd_AR\\\\Alembic_Pharmaceuticals_Ltd_AR_24161_APLLTD_2023_2024_21062024172948.pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                     | 60/179 [00:52<02:20,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded: nse_annual_reports\\Alkem_Laboratories_Ltd_AR\\AR.pdf\n",
      "‚ö†Ô∏è Skipping Alkem Laboratories Ltd. due to unexpected error: [WinError 183] Cannot create a file when that file already exists: 'nse_annual_reports\\\\Alkem_Laboratories_Ltd_AR\\\\AR.pdf' -> 'nse_annual_reports\\\\Alkem_Laboratories_Ltd_AR\\\\Alkem_Laboratories_Ltd_AR_24674_ALKEM_2023_2024_2607202416257.pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                     | 60/179 [00:58<01:56,  1.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 187\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[43mfetch_nse_reports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSymbol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompany Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(PROGRESS_FILE,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    189\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(company_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 164\u001b[0m, in \u001b[0;36mfetch_nse_reports\u001b[1;34m(symbol, company_name)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m reports:\n\u001b[0;32m    163\u001b[0m         report_url \u001b[38;5;241m=\u001b[39m report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 164\u001b[0m         \u001b[43mdownload_and_process_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è No reports found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 122\u001b[0m, in \u001b[0;36mdownload_and_process_report\u001b[1;34m(company_name, report_url)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 122\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Downloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\urllib3\\response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\urllib3\\response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\urllib3\\response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Define User-Agent list at the beginning\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# --- Step 1: Launch Browser and Extract Cookies ---\n",
    "def get_nse_cookies():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--start-maximized\")  # Open in full screen\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    \n",
    "    options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
    "\n",
    "    # Start WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    nse_url = \"https://www.nseindia.com/companies-listing/corporate-filings-annual-reports\"\n",
    "    driver.get(nse_url)\n",
    "    \n",
    "    # Wait for JavaScript elements to load\n",
    "    time.sleep(random.randint(3, 6))\n",
    "\n",
    "    # Scroll to simulate user activity\n",
    "    driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "    # Extract cookies\n",
    "    cookies = {cookie['name']: cookie['value'] for cookie in driver.get_cookies()}\n",
    "    \n",
    "    driver.quit()  # Close browser\n",
    "    return cookies\n",
    "\n",
    "# --- Step 2: Load Stock List ---\n",
    "df_stocks = pd.read_csv(\"../data/top_250_stocks.csv\")  # Ensure this file exists\n",
    "\n",
    "BASE_FOLDER = \"nse_annual_reports\"\n",
    "os.makedirs(BASE_FOLDER, exist_ok=True)\n",
    "\n",
    "# --- Step 3: Create Session Using Extracted Cookies ---\n",
    "cookies = get_nse_cookies()\n",
    "\n",
    "session = requests.Session()\n",
    "for name, value in cookies.items():\n",
    "    session.cookies.set(name, value)\n",
    "\n",
    "# Define headers after USER_AGENTS is initialized\n",
    "headers = {\n",
    "    \"accept\": \"*/*\",\n",
    "    \"user-agent\": random.choice(USER_AGENTS),\n",
    "    \"referer\": \"https://www.nseindia.com/companies-listing/corporate-filings-annual-reports\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Cookies extracted and session initialized successfully.\")\n",
    "\n",
    "# --- Step 4: Functions for Downloading and Processing Reports ---\n",
    "def clean_filename(name,max_length=50):\n",
    "    \"\"\"Sanitize company names for filenames.\"\"\"\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)  # Remove special characters\n",
    "    name = name.strip().replace(\" \", \"_\")  # Replace spaces with underscores\n",
    "    return name[:max_length]  # Limit name length to avoid issues\n",
    "\n",
    "def safe_request(url, max_retries=5):\n",
    "    \"\"\"Retry requests with exponential backoff.\"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, timeout=15, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code in [403, 429]:  # Forbidden or Too Many Requests\n",
    "                wait_time = 30 + (10 * attempt)\n",
    "                print(f\"‚ö†Ô∏è Server blocking requests. Waiting {wait_time}s before retrying...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected HTTP {response.status_code}. Retrying...\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Request failed: {e}. Retrying...\")\n",
    "\n",
    "        attempt += 1\n",
    "        time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "    print(f\"‚ùå Failed after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# def download_and_process_report(company_name, report_url):\n",
    "#     \"\"\"Download, extract, and save PDFs.\"\"\"\n",
    "#     clean_company_name = clean_filename(company_name)\n",
    "#     folder_path = os.path.join(BASE_FOLDER, f\"{clean_company_name}_AR\")\n",
    "#     os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "#     filename = report_url.split(\"/\")[-1]\n",
    "#     base_filename=clean_filename(filename.split(\"_\")[0],max_length=20)\n",
    "#     file_path = os.path.join(folder_path, f\"{base_filename}.pdf\")\n",
    "\n",
    "#     response = safe_request(report_url)\n",
    "#     if response:\n",
    "#         with open(file_path, \"wb\") as file:\n",
    "#             for chunk in response.iter_content(chunk_size=1024):\n",
    "#                 file.write(chunk)\n",
    "#         print(f\"‚úÖ Downloaded: {file_path}\")\n",
    "\n",
    "#         if filename.endswith(\".zip\"):\n",
    "#             try:\n",
    "#                 with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "#                     zip_ref.extractall(folder_path)\n",
    "#                 print(f\"üìÇ Extracted: {file_path} to {folder_path}\")\n",
    "\n",
    "#                 for file in os.listdir(folder_path):\n",
    "#                     if file.endswith(\".pdf\"):\n",
    "#                         os.rename(os.path.join(folder_path, file), os.path.join(folder_path, f\"{clean_company_name}_{file}\"))\n",
    "#                         print(f\"üìÑ Saved PDF: {clean_company_name}_{file}\")\n",
    "\n",
    "#                 os.remove(file_path)  # Remove ZIP after extraction\n",
    "\n",
    "#             except zipfile.BadZipFile:\n",
    "#                 print(f\"‚ùå Corrupted ZIP file: {file_path}\")\n",
    "\n",
    "#         elif filename.endswith(\".pdf\"):\n",
    "#             os.rename(file_path, os.path.join(folder_path, f\"{clean_company_name}_{filename}\"))\n",
    "#             print(f\"üìÑ Directly saved PDF: {clean_company_name}_{filename}\")\n",
    "\n",
    "#     else:\n",
    "#         print(f\"‚ùå Failed to download: {report_url}\")\n",
    "def extract_year_from_filename(filename):\n",
    "    \"\"\"Extract the year from the filename using regex.\"\"\"\n",
    "    match = re.search(r'(\\d{4}_\\d{4})', filename)  # Matches patterns like '2023_2024'\n",
    "    return match.group(1) if match else \"Unknown_Year\"\n",
    "\n",
    "def download_and_process_report(company_name, report_url):\n",
    "    \"\"\"Download, extract, and save PDFs with correct naming to avoid duplicates.\"\"\"\n",
    "    clean_company_name = clean_filename(company_name)\n",
    "    folder_path = os.path.join(BASE_FOLDER, f\"{clean_company_name}_AR\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    filename = report_url.split(\"/\")[-1]\n",
    "    financial_year = extract_year_from_filename(filename)\n",
    "    base_filename = clean_filename(filename.split(\"_\")[0], max_length=20)  # Shorten original filename\n",
    "\n",
    "    unique_filename = f\"{clean_company_name}_{financial_year}.pdf\"\n",
    "    file_path = os.path.join(folder_path, unique_filename)\n",
    "\n",
    "    response = safe_request(report_url)\n",
    "    if response:\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        print(f\"‚úÖ Downloaded: {file_path}\")\n",
    "\n",
    "        if filename.endswith(\".zip\"):\n",
    "            try:\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(folder_path)\n",
    "                print(f\"üìÇ Extracted: {file_path} to {folder_path}\")\n",
    "\n",
    "                for file in os.listdir(folder_path):\n",
    "                    if file.endswith(\".pdf\"):\n",
    "                        extracted_pdf_path = os.path.join(folder_path, file)\n",
    "                        new_pdf_path = os.path.join(folder_path, f\"{clean_company_name}_{financial_year}.pdf\")\n",
    "\n",
    "                        # Ensure we don't overwrite an existing file\n",
    "                        counter = 1\n",
    "                        while os.path.exists(new_pdf_path):\n",
    "                            new_pdf_path = os.path.join(folder_path, f\"{clean_company_name}_{financial_year}_{counter}.pdf\")\n",
    "                            counter += 1\n",
    "\n",
    "                        os.rename(extracted_pdf_path, new_pdf_path)\n",
    "                        print(f\"üìÑ Renamed PDF: {new_pdf_path}\")\n",
    "\n",
    "                os.remove(file_path)  # Remove ZIP after extraction\n",
    "\n",
    "            except zipfile.BadZipFile:\n",
    "                print(f\"‚ùå Corrupted ZIP file: {file_path}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download: {report_url}\")\n",
    "def fetch_nse_reports(symbol, company_name):\n",
    "    \"\"\"Fetch report links from NSE API.\"\"\"\n",
    "    encoded_company_name = company_name.replace(\" \", \"%20\")\n",
    "    nse_api_url = f\"https://www.nseindia.com/api/annual-reports?index=equities&symbol={symbol}&issuer={encoded_company_name}\"\n",
    "\n",
    "    response = safe_request(nse_api_url)\n",
    "\n",
    "    if response:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            reports = data.get(\"data\", [])\n",
    "\n",
    "            if reports:\n",
    "                for report in reports:\n",
    "                    report_url = report[\"fileName\"]\n",
    "                    download_and_process_report(company_name, report_url)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No reports found for {company_name} ({symbol})\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå Error parsing JSON response for {company_name} ({symbol})\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to fetch reports for {company_name} ({symbol})\")\n",
    "PROGRESS_FILE=\"./progress.txt\"\n",
    "if os.path.exists(PROGRESS_FILE):\n",
    "    with open(PROGRESS_FILE,\"r\") as f:\n",
    "        completed_companies=set(line.strip() for line in f.readlines())\n",
    "else:\n",
    "    completed_companies=set()\n",
    "# --- Step 5: Loop Through Stocks and Fetch Reports ---\n",
    "for _, row in tqdm(df_stocks.iterrows(), total=len(df_stocks)):\n",
    "    company_name=row[\"Company Name\"]\n",
    "    symbol=row[\"Symbol\"]\n",
    "    if company_name in completed_companies:\n",
    "        print(f\" Skipping {company_name},already processed\")\n",
    "        continue\n",
    "    try:\n",
    "        fetch_nse_reports(row[\"Symbol\"], row[\"Company Name\"])\n",
    "        with open(PROGRESS_FILE,\"a\") as f:\n",
    "            f.write(company_name+\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {row['Company Name']} due to unexpected error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All reports downloaded, extracted, and organized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92b49b-213e-4686-9cbb-519df5460419",
   "metadata": {},
   "outputs": [],
   "source": [
    "try to make it such that if os error arises it ignores that company and goes to next company"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
